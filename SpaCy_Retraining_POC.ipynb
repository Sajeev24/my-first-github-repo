{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5JHW160M7RJ4"
   },
   "source": [
    "We are trying to build a POC to prove if retraining SpaCy's pretrained model on private data improves the performance. This logically seems so. But need to see by how much does it improve.\n",
    "\n",
    "One hurdle in this process is that the annotation labels that SpaCy's NER model is trained is different than that of our dataset. So, we would need to find the equivalence between the labels to make this happen. You can find the labels that SpaCy parses on its [documentation page](https://spacy.io/api/annotation#named-entities). Whereas  [our dataset](https://www.kaggle.com/alaakhaled/conll003-englishversion) has these labels: PER, ORG, LOC, MISC\n",
    "\n",
    "The following are the dependencies of this notebook. Please make sure they are installed before running this notebook:\n",
    "* SpaCy\n",
    "* Sklearn\n",
    "\n",
    "We need to compare the performance on the following scenarios:\n",
    "* Performance on spaCy's vanilla pretrained model 'en_core_web_md'\n",
    "* Performance after retraining vanilla model ('en_core_web_md') on training dataset.\n",
    "* Performance on a model obtained by training from scratch on a training dataset.\n",
    "* Performance on spaCy's vanilla pretrained model 'en_core_web_lg' which is the <b>largest</b> model\n",
    "* Performance after retraining vanilla model ('en_core_web_lg') on training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKpviA9-7RJ6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import spacy \n",
    "import random\n",
    "import copy \n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from spacy.gold import GoldParse\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "in_ipythonkernel = 'ipykernel' in sys.modules\n",
    "in_collab = 'google.colab' in sys.modules\n",
    "if in_ipythonkernel == True:\n",
    "    in_jupyter = in_collab == False\n",
    "print('Google Collab :       ', in_collab)\n",
    "print('In Jupyter Notebook : ', in_jupyter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pX_hig8ATG9u"
   },
   "source": [
    "**Convert dataset files in NER format to JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "ouxmkZ_VRzzW",
    "outputId": "de050cc6-db3f-4370-bf8e-fad3dcbf45f2"
   },
   "outputs": [],
   "source": [
    "if in_jupyter == True:\n",
    "    ! python -m spacy convert -c ner valid.txt > valid.json\n",
    "    ! python -m spacy convert -c ner test.txt > test.json\n",
    "    ! python -m spacy convert -c ner train.txt > train.json\n",
    "elif in_collab == True:\n",
    "    ! mkdir dir\n",
    "    ! python -m spacy convert -c ner valid.txt dir\n",
    "    ! python -m spacy convert -c ner test.txt dir\n",
    "    ! python -m spacy convert -c ner train.txt dir\n",
    "    ! mv dir/* . \n",
    "    ! rmdir dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_FBWx_b2J-F"
   },
   "source": [
    "# **Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOzFHPDc2ZkZ"
   },
   "outputs": [],
   "source": [
    "if in_collab == True:\n",
    "    from IPython.display import display as print\n",
    "\n",
    "# Define a function to convert the ConLL ner data to a format that spaCy understands\n",
    "def convert_conll_ner_to_spacy(json_obj):\n",
    "    if in_collab == True:\n",
    "        return convert_conll_ner_to_spacy_google_collab_(json_obj)\n",
    "    else:\n",
    "        assert in_jupyter == True\n",
    "        return convert_conll_ner_to_spacy_jupyter_nbook_(json_obj)\n",
    "        \n",
    "def process_sentence_token_(sentence_tokens):\n",
    "    end = None\n",
    "    byte_count = 0\n",
    "    entities = []\n",
    "    set1 = set()\n",
    "    for i,token in enumerate(sentence_tokens):\n",
    "        #print('\\n\\t' + str(token))\n",
    "        byte_count += len(token['orth'])\n",
    "        ner_tag_scheme_list = token['ner'].split('-')\n",
    "        biluo_scheme = ner_tag_scheme_list[0]\n",
    "        ner_tag = None\n",
    "\n",
    "        if biluo_scheme == 'O':\n",
    "            #print(f'\\tbc:{byte_count}')\n",
    "            pass\n",
    "        else: \n",
    "            set1.add(token['ner'].split('-')[1])\n",
    "            ner_tag = ner_tag_scheme_list[1]\n",
    "            if biluo_scheme == 'B':\n",
    "                start = byte_count - len(token['orth'])\n",
    "            elif biluo_scheme == 'I':\n",
    "                pass\n",
    "            elif biluo_scheme == 'L':\n",
    "                end = byte_count\n",
    "            elif biluo_scheme == 'U':\n",
    "                start = byte_count - len(token['orth'])\n",
    "                end = byte_count\n",
    "        byte_count += 1      # For a single space between tokens\n",
    "        if end != None:\n",
    "            #print(f'\\ttoken:{full[start:end]} -- start:{start} end:{end} bc:{byte_count} -- tag:{ner_tag}')\n",
    "            entities.append((start, end, ner_tag))\n",
    "            end = None\n",
    "    return entities\n",
    "    \n",
    "\n",
    "def convert_conll_ner_to_spacy_google_collab_(json_obj):\n",
    "    training_data = []\n",
    "    sentences = json_obj[0]['paragraphs'][0]['sentences']\n",
    "    for j,sentence in enumerate(sentences):\n",
    "        byte_count = 0\n",
    "        sentence_tokens = sentence['tokens']\n",
    "        full = \" \".join([token['orth'] for token in sentence_tokens])\n",
    "        entities = process_sentence_token_(sentence_tokens)\n",
    "        training_data.append((full, {\"entities\" : entities}))\n",
    "    #print(set1)\n",
    "    return training_data\n",
    "\n",
    "def convert_conll_ner_to_spacy_jupyter_nbook_(json_obj):\n",
    "    training_data = []\n",
    "    for j,document in enumerate(json_obj):\n",
    "        sentence_tokens = document['paragraphs'][0]['sentences'][0]['tokens']\n",
    "        #print(sentence_tokens)\n",
    "        full = \" \".join([token['orth'] for token in sentence_tokens])\n",
    "        entities = process_sentence_token_(sentence_tokens)\n",
    "        training_data.append((full, {\"entities\" : entities}))\n",
    "    #print(set1)\n",
    "    return training_data\n",
    "\n",
    "\n",
    "# Take convert human annotated examples in spacy format and build a dictionary \n",
    "# that maps annotation labels to annotated text. This wil help up in peering \n",
    "# into the annotations to find what the labels actually mean.\n",
    "def spacy_get_annotations_by_labels(examples_in_spacy_fmt, labels='all'):\n",
    "    if labels != 'all':\n",
    "        raise('Not implemented for specific label')\n",
    "    label_to_text_map =  {} \n",
    "    for text,annotations in examples_in_spacy_fmt:\n",
    "        entities = annotations['entities']\n",
    "        #print(text)\n",
    "        for (start, end, label) in entities:\n",
    "            #print('\\t',start, end, label, '\\''+text[start:end]+'\\'')\n",
    "            if label not in label_to_text_map:\n",
    "                label_to_text_map[ label ] = set()\n",
    "            else:\n",
    "                label_to_text_map[ label ].add( text[start:end] )\n",
    "    return label_to_text_map\n",
    "\n",
    "# To display the this annotations label to text dictionary\n",
    "def display_labels2text_dict(dictionary, num_samples_per_entity):\n",
    "    temp_dict = {}\n",
    "    for key, set_ in dictionary.items():\n",
    "        temp_dict[key] = []\n",
    "        set_list = list(set_)\n",
    "        print(f'set_len({key}) = {len(set_)}')\n",
    "        if len(set_list) < num_samples_per_entity:\n",
    "            [ temp_dict[key].append(e) for e in set_list ]\n",
    "        else: \n",
    "            for i in range(num_samples_per_entity):\n",
    "                temp_dict[key].append( set_list[ int(random.random() * len(set_list)) ] )\n",
    "    pprint(temp_dict, width=200)\n",
    "\n",
    "def copy_dict(src_dict, dest_dict):\n",
    "    for label,set_ in src_dict.items():\n",
    "        if label not in dest_dict:\n",
    "            dest_dict[label] = set()\n",
    "        else:\n",
    "            [ dest_dict[label].add(i) for i in src_dict[label] ]\n",
    "    return\n",
    "    \n",
    "# Help merging the dictionaries for training and test examples\n",
    "def merge_labels_to_text_dict(dict1, dict2):\n",
    "    merged_dict = {}\n",
    "    copy_dict(dict1, merged_dict)\n",
    "    copy_dict(dict2, merged_dict)\n",
    "    return merged_dict\n",
    "\n",
    "import random\n",
    "\n",
    "# Dictionary that maps entities in the pretrained model (en_core_web_xx) to the unique texts of the test set\n",
    "# they resolve to. So that we know what all texts match a particular entity label. This will help us \n",
    "# in understanding which label maps to the labels of the dataset that is used for pre-training.\n",
    "def spacy_ner_predictions_to_dict(examples_in_spacy_fmt, model):\n",
    "    pred_ent_to_text_map = {} \n",
    "    for text,_ in examples_in_spacy_fmt:\n",
    "        pred_doc = model(text)\n",
    "        for ent in pred_doc.ents: \n",
    "            if ent.label_ not in pred_ent_to_text_map:\n",
    "                pred_ent_to_text_map[ ent.label_ ] = set()\n",
    "            else:\n",
    "                pred_ent_to_text_map[ ent.label_ ].add( ent.text )\n",
    "    return pred_ent_to_text_map\n",
    "\n",
    "def print_annotaions_and_predictions(spacy_examples):\n",
    "    for text,annotation in spacy_examples:\n",
    "        pred_doc = pretrained_nlp(text)\n",
    "        ypred = [ (ent.label_,ent.text) for ent in pred_doc.ents ]\n",
    "        \n",
    "        annot_list = [( ent[2], text[ent[0]:ent[1]] ) for ent in annotation['entities']]\n",
    "        \n",
    "        print('Human annotated: ', annot_list)\n",
    "        print('Predictions    : ', ypred)\n",
    "        print('\\n')\n",
    "\n",
    "def map_pred_tag_to_domain(pred_bilou_tag, equivalence_map):\n",
    "    if pred_bilou_tag[0] == 'O':\n",
    "        return 'O'\n",
    "    bilou_part = pred_bilou_tag.split('-')[0]\n",
    "    label_part = pred_bilou_tag.split('-')[1]\n",
    "        \n",
    "    if label_part not in equivalence_map.keys():\n",
    "        return 'O'\n",
    "    return bilou_part + '-' + equivalence_map[label_part]\n",
    "\n",
    "def convert_doc_to_bilou_tags(doc):\n",
    "    list_ = [] \n",
    "    for i in range(len(doc)):\n",
    "        # Process BILOU tag\n",
    "        if doc[i].ent_iob_ == 'O':\n",
    "            bilou_tag = 'O'\n",
    "        else:\n",
    "            if doc[i].ent_iob_ == 'B':\n",
    "                bilou_tag = 'U' if (i+1) < len(doc) and doc[i+1].ent_iob_ != 'I' else 'B'\n",
    "            elif doc[i].ent_iob_ == 'I':\n",
    "                bilou_tag = 'I' if (i+1) < len(doc) and doc[i+1].ent_iob_ == 'I' else 'L'\n",
    "            else:\n",
    "                assert \"This is unexpected\"\n",
    "        bilou_tag = 'O' if doc[i].ent_type_ == '' else bilou_tag + '-' + doc[i].ent_type_\n",
    "        \n",
    "        list_.append( (bilou_tag, doc[i].text) )    \n",
    "    #print('--->> ',list_)\n",
    "    return list_\n",
    "\n",
    "def perf_measure(y_actual, y_hat, label):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_hat)):\n",
    "        if y_actual[i]==y_hat[i]==label:\n",
    "            TP += 1\n",
    "        if y_hat[i]==label and y_actual[i]!=label:\n",
    "            FP += 1\n",
    "        if y_actual[i]!=label and y_hat[i]!=label:\n",
    "            TN += 1\n",
    "        if y_hat[i]!=label and y_actual[i]==label:\n",
    "            FN += 1\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "\n",
    "def compute_scores(spacy_examples, model, label_map):\n",
    "    perf_stats_per_tag = { }\n",
    "    for text,annotation in spacy_examples:\n",
    "        doc = model.make_doc(text)\n",
    "        gold = GoldParse(doc, entities=annotation['entities'])\n",
    "        gold_tag_list = [i for i in zip(gold.ner,gold.words)]\n",
    "        #print('\\nGold       : ', gold_tag_list)\n",
    "        \n",
    "        ner_tag_predict_doc = model(text)\n",
    "        ner_tag_predict_list = convert_doc_to_bilou_tags(ner_tag_predict_doc)\n",
    "        ner_tag_predict_list = list( map(lambda e: (map_pred_tag_to_domain(e[0],PRED_LABELS_EQUIV_MAP), e[1]), \n",
    "                                         ner_tag_predict_list) \n",
    "                                   )\n",
    "        #print(  'Predicted  : ', ner_tag_predict_list)\n",
    "        \n",
    "        #for i in range(len(ner_tag_predict_list)):\n",
    "        #    if ner_tag_predict_list[i][0] != gold_tag_list[i][0]:\n",
    "        #        print('\\t',ner_tag_predict_list[i], gold_tag_list[i])\n",
    "        #        continue\n",
    "        #    if ner_tag_predict_list[i][1] != gold_tag_list[i][1]:\n",
    "        #        print('\\t',ner_tag_predict_list[i], gold_tag_list[i])\n",
    "        #        continue\n",
    "        \n",
    "        # Compute unique labels and populate y_true and y_pred\n",
    "        unique_labels = set()\n",
    "        y_true, y_pred = [],[]\n",
    "        for t in gold_tag_list:\n",
    "            if t[0] != 'O':\n",
    "                unique_labels.add( t[0] )\n",
    "            y_true.append( t[0] )\n",
    "        for t in ner_tag_predict_list:\n",
    "            if t[0] != 'O':\n",
    "                unique_labels.add( t[0] )\n",
    "            y_pred.append( t[0] )\n",
    "            \n",
    "        #print('\\tUnique Labels :', unique_labels)\n",
    "        #print('\\ty_true        :', y_true)\n",
    "        #print('\\ty_pred        :', y_pred)\n",
    "        for label in unique_labels:\n",
    "            (TP, FP, TN, FN) = perf_measure(y_true, y_pred, label)\n",
    "            CNT = len(y_true)\n",
    "            #print(label,' ',f'(TP:{TP}, FP:{FP}, TN:{TN}, FN:{FN}, CNT:{CNT})')\n",
    "            \n",
    "            label_part = label.split('-')[1]\n",
    "            if label_part not in perf_stats_per_tag:\n",
    "                perf_stats_per_tag[label_part] = {'TP':0, 'FP':0, 'TN':0, 'FN':0, 'CNT':0}\n",
    "                \n",
    "            perf_stats_per_tag[label_part]['TP'] += TP\n",
    "            perf_stats_per_tag[label_part]['FP'] += FP\n",
    "            perf_stats_per_tag[label_part]['TN'] += TN\n",
    "            perf_stats_per_tag[label_part]['FN'] += FN\n",
    "            perf_stats_per_tag[label_part]['CNT'] += CNT\n",
    "    return perf_stats_per_tag\n",
    "\n",
    "def display_perf_stats_per_tag( stats_per_tag ):\n",
    "    # Now compute the scores\n",
    "    pprint(stats_per_tag)\n",
    "    for tag,st in stats_per_tag.items():\n",
    "        print(f'For label: \"{tag}\"')\n",
    "        accuracy = (st['TP'] + st['TN']) / st['CNT']\n",
    "        print(\"\\tAccuracy : \"  + str(accuracy * 100) + \"%\")\n",
    "        \n",
    "        precision = 0\n",
    "        if (st['TP'] + st['FP']) != 0:\n",
    "            precision = st['TP'] / (st['TP'] + st['FP'])\n",
    "        print(\"\\tPrecision : \" + str(precision))\n",
    "        \n",
    "        recall = 0\n",
    "        if (st['TP'] + st['FN']) != 0:\n",
    "            recall = st['TP'] / (st['TP'] + st['FN'])\n",
    "        print(\"\\tRecall : \"    + str(recall))\n",
    "        \n",
    "        fscore = 0\n",
    "        if (precision + recall) != 0:\n",
    "            fscore = (2 * precision * recall) / (precision + recall)\n",
    "        print(\"\\tF-score : \"   + str(fscore))\n",
    "        \n",
    "\n",
    "def conv_dataset_to_match_domain(spacy_examples, dataset_to_model_tag_map):\n",
    "    spacy_examples = copy.deepcopy(spacy_examples)\n",
    "    for text,annotations in spacy_examples:\n",
    "        entities = annotations['entities']\n",
    "        for i,ent in enumerate(entities):\n",
    "            if ent[2][0] == 'O':\n",
    "                continue\n",
    "            entities[i] = (ent[0],ent[1],dataset_to_model_tag_map[ent[2]])                                        \n",
    "    return spacy_examples\n",
    "\n",
    "# The 'model' parameter could either be a pretrained model. Default behavior is to \n",
    "# training from scratch. \n",
    "def train_spacy_model(train_examples, model=None):\n",
    "    nlp = model  # create blank Language class\n",
    "    if nlp == None:\n",
    "        nlp = spacy.blank('en')\n",
    "    \n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if model == None:\n",
    "        if 'ner' not in nlp.pipe_names:\n",
    "            ner = nlp.create_pipe('ner')\n",
    "            nlp.add_pipe(ner, last=True)\n",
    "        # add labels\n",
    "        for _, annotations in train_examples:\n",
    "             for ent in annotations.get('entities'):\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(10):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(train_examples)\n",
    "            losses = {}\n",
    "            for text, annotations in train_examples:\n",
    "                try:\n",
    "                    nlp.update(\n",
    "                        [text],         # batch of texts\n",
    "                        [annotations],  # batch of annotations\n",
    "                        drop=0.2,       # dropout - make it harder to memorise data\n",
    "                        sgd=optimizer,  # callable to update weights\n",
    "                        losses=losses)\n",
    "                except:\n",
    "                    continue\n",
    "                    #print('Hello ' + str(sys.exc_info()))\n",
    "            print(losses)\n",
    "    return nlp\n",
    "\n",
    "# Serialize model\n",
    "def save_model_to_file(model, file_name):\n",
    "    model_bytes = model.to_bytes()\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(model_bytes)\n",
    "        f.flush()\n",
    "\n",
    "# Deserialize\n",
    "def load_model_from_file(file_name, nlp_load_into=None):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        read_bytes = f.read()\n",
    "    print(f'#Bytes-read: {len(read_bytes)}')\n",
    "    #if nlp_load_into == None:\n",
    "    #    nlp_load_into = spacy.load('en_core_web_md')\n",
    "    #    #nlp_load_into.remove_pipe('ner')\n",
    "    #    #for pipe_name in ['tagger', 'parser', 'ner']:\n",
    "    #    #    if pipe_name not in nlp_load_into.pipe_names:\n",
    "    #    #        pipe = nlp_load_into.create_pipe(pipe_name)\n",
    "    #    #        nlp_load_into.add_pipe(pipe)\n",
    "    nlp_load_into.from_bytes(read_bytes)\n",
    "    return nlp_load_into\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uO894N8U4O54"
   },
   "source": [
    "# **Working with datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWovb_Bg7RJ-"
   },
   "source": [
    "**Loading datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fdn8DzDl7RJ_",
    "outputId": "0fe58bae-a678-4758-cc5b-f016ddc01e82"
   },
   "outputs": [],
   "source": [
    "with open('./train.json', 'r') as f:\n",
    "    read_bytes = f.read()\n",
    "print(len(read_bytes))\n",
    "train_obj = json.loads(read_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SlEMXIUH7RKQ",
    "outputId": "6cde63bf-95ac-4588-a03a-0389326cec43"
   },
   "outputs": [],
   "source": [
    "with open('./test.json', 'r') as f:\n",
    "    read_bytes = f.read()\n",
    "print(len(read_bytes))\n",
    "test_obj = json.loads(read_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvd_h-dH7RKG"
   },
   "source": [
    "**Convert dataset json docs into a format that spaCy understands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6aqi1MO7RKH"
   },
   "outputs": [],
   "source": [
    "training_examples = convert_conll_ner_to_spacy(train_obj)\n",
    "test_examples = convert_conll_ner_to_spacy(test_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_0Nx4UCGS54F"
   },
   "outputs": [],
   "source": [
    "for t in (training_examples + test_examples)[0:1000]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CtUMMkhi7RKJ"
   },
   "source": [
    "# **Download and Load pretrained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "zLApjS-77RKK",
    "outputId": "2cec703a-6935-4f18-ca61-56ecdf559573"
   },
   "outputs": [],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "pretrained_nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lClWsYy17RKZ"
   },
   "source": [
    "# **Exploratory analysis of the dataset to find the texts corresponding different labels**\n",
    "\n",
    "The NER entities of SpaCy's model and the manually annotated labels of the dataset dont match. So we need to map those labels to the closest entities that SpaCy's NER model deals with. \n",
    "\n",
    "This analysis will help us figure out the intention of the human annotator such that we can then be able to map the labels from the dataset's domain to the closest labels that the model was trained on. \n",
    "\n",
    "Labels is the dataset:\n",
    "* 'LOC'\n",
    "* 'PER'\n",
    "* 'ORG'\n",
    "* 'MISC'\n",
    "\n",
    "Labels that the model was trained with:\n",
    "* 'NORP'       \n",
    "* 'WORK_OF_ART'\n",
    "* 'FAC'        \n",
    "* 'PRODUCT'    \n",
    "* 'EVENT'      \n",
    "* 'GPE'        \n",
    "* 'LOC'        \n",
    "* 'ORG'        \n",
    "* 'PERSON'     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0ttKacx7RKW"
   },
   "source": [
    "**Printing annotated texts corresponding labels in the dataset's domain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OgeSi6la7RKX",
    "outputId": "4faa1efa-740a-4dfb-cb12-8b34c9abb5ce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels2text_dict_train    = spacy_get_annotations_by_labels(training_examples)\n",
    "labels2text_dict_test     = spacy_get_annotations_by_labels(test_examples)\n",
    "merged_dict               = merge_labels_to_text_dict(labels2text_dict_train, labels2text_dict_test)\n",
    "\n",
    "label_ = 'MISC'  # Query this labels\n",
    "display_labels2text_dict(merged_dict, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmCY4AkV0TB-"
   },
   "source": [
    "**Printing annotated texts corresponding labels in the model's domain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pretrained_nlp('Apple is a fruit')\n",
    "print(pretrained_nlp.pipe_names)\n",
    "for t in doc.ents: print(t.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "a3ZxkQGf7RKa",
    "outputId": "50679b9e-33a1-448c-f82d-78d2f7e93d74",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Following call takes time. Uncommnet if contents of the parameter has changed\n",
    "pred_dict = spacy_ner_predictions_to_dict(test_examples, pretrained_nlp)\n",
    "\n",
    "display_labels2text_dict(pred_dict, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONnBP8KXVepd"
   },
   "source": [
    "**Post analysis mapping of labels from the dataset's domain to the model's domain**\n",
    "It was found after analysis that :\n",
    "* the label MISC from the dataset's domain map roughly to model domain labels: 'NORP', 'WORK_OF_ART', 'FAC', 'PRODUCT', 'EVENT'.\n",
    "* Dataset label 'LOC' can be mapped to model label 'GPE'/'LOC'\n",
    "* Dataset label 'ORG' can be mapped to model label 'ORG'\n",
    "* Dataset label 'PER' can be mapped to model label 'PERSON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0EeqtGp7RKg"
   },
   "outputs": [],
   "source": [
    "PRED_LABELS_EQUIV_MAP = {\n",
    "    'NORP'       : 'MISC',\n",
    "    'WORK_OF_ART': 'MISC',\n",
    "    'FAC'        : 'MISC',\n",
    "    'PRODUCT'    : 'MISC',\n",
    "    'EVENT'      : 'MISC',\n",
    "    'GPE'        : 'LOC',\n",
    "    'LOC'        : 'LOC',\n",
    "    'ORG'        : 'ORG', \n",
    "    'PERSON'     : 'PER'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sjHd1o4a7RKi"
   },
   "source": [
    "# **Evaluating on spaCy's pretrained, medium, vanilla model**\n",
    "\n",
    "This score would be compared with the score on the same model which has been retrained on the training data. We count the\n",
    "* Number of true positives  - Labelled correctly\n",
    "* Number of false positives - Labelled and did not got it correct AND those that should not have been labelled.\n",
    "* Number of false negatives - Those that were not labelled at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "hJzH_cxO7RKj",
    "outputId": "27e78b04-6b1e-4b50-9091-e7eb41a1d3aa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print_annotaions_and_predictions(test_examples)\n",
    "stats_per_tag = compute_scores(test_examples, pretrained_nlp, PRED_LABELS_EQUIV_MAP)\n",
    "display_perf_stats_per_tag(stats_per_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAgEQdC57RKm"
   },
   "source": [
    "# **Retraining pretrained-medium SpaCy model and evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert dataset to match domain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_model_tag_map = { 'MISC': 'NORP',\n",
    "                             'LOC' : 'LOC',\n",
    "                             'ORG' : 'ORG',\n",
    "                             'PER' : 'PERSON'}\n",
    "mapped_examples = conv_dataset_to_match_domain(training_examples, \n",
    "                                               dataset_to_model_tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "gr62Chk_7RKn",
    "outputId": "928b42a6-6bc8-4bd0-884b-2c54f75e9406",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_spacy_model(mapped_examples, pretrained_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XX7oiws7RKp"
   },
   "source": [
    "**Evaluating the retrained model on test data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "E6aIIvHe7RKq",
    "outputId": "8f345e3b-7b5a-4082-fd2d-c824f1b79f99"
   },
   "outputs": [],
   "source": [
    "--allow-root#nlp_md_retrained = copy.deepcopy(pretrained_nlp) # *ExpensiveResource*. comment after execution\n",
    "\n",
    "#save_model_to_file(nlp_md_retrained, 'saved_models/retrained__md_spacy_model.bin')\n",
    "\n",
    "loaded = load_model_from_file('saved_models/retrained__md_spacy_model.bin', \n",
    "                              spacy.load('en_core_web_md'))\n",
    "\n",
    "stats_per_tag = compute_scores(test_examples, \n",
    "                               loaded,          ## Need to review before executing the cell\n",
    "                               PRED_LABELS_EQUIV_MAP)\n",
    "display_perf_stats_per_tag(stats_per_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aaO9dQ47RKu"
   },
   "source": [
    "# **Training a SpaCy model from scratch and its evaluation**\n",
    "\n",
    "Now lets train a model from sratch and compute the score. The idea here would be to see if the performance on the retrained spaCy model is better than that of on a model that has been trained from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "colab_type": "code",
    "id": "7Idaj1sd7RKv",
    "outputId": "817d813f-8cd6-4b91-d879-66621282e3d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nlp_from_scratch = train_spacy_model(training_examples)   # No model specified i.e spacy.blank(...)\n",
    "\n",
    "#save_model_to_file(nlp_from_scratch, 'saved_models/space_model_from_scratch.bin')\n",
    "\n",
    "loaded =  load_model_from_file('saved_models/space_model_from_scratch.bin')\n",
    "\n",
    "stats_per_tag = compute_scores(test_examples, \n",
    "                               loaded,     ## Need to review before executing the cell\n",
    "                               PRED_LABELS_EQUIV_MAP)\n",
    "display_perf_stats_per_tag(stats_per_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MPHtFU7620Y"
   },
   "source": [
    "# **Evaluating on SpaCy's pretrained, large, vanilla model**\n",
    "\n",
    "Download spaCy's large model (en_core_web_lg) and find how it performs on the test examples. Get the performance scores.\n",
    "\n",
    "Later, retrain this \"large\" model and see how it compares with the vanilla and the retrained results on \"medium\" model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "D1X9P9EO61eM",
    "outputId": "5ca176d1-39c8-4321-f649-c425bd22c7b7"
   },
   "outputs": [],
   "source": [
    "# Downloading large model\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp_lg_pretrained = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "EG9NbWmM8F62",
    "outputId": "4f672890-68f4-4bff-9302-a0e29d70428e"
   },
   "outputs": [],
   "source": [
    "stats_per_tag = compute_scores(test_examples, nlp_lg_pretrained, PRED_LABELS_EQUIV_MAP)\n",
    "display_perf_stats_per_tag(stats_per_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZ_76m758UKy"
   },
   "source": [
    "# **Retraining pretrained-large SpaCy model and evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "iZz_dwSG8aWA",
    "outputId": "ca66ecac-e002-4ac3-d206-4b618909a3b7"
   },
   "outputs": [],
   "source": [
    "train_spacy_model(mapped_examples, pretrained_nlp) # Comment after use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "nmwkvGZ98xez",
    "outputId": "bb6df8fa-562c-45a6-c26f-17acdd877363"
   },
   "outputs": [],
   "source": [
    "#nlp_retrained_lg = copy.deepcopy(pretrained_nlp) # *ExpensiveResource*. comment after execution\n",
    "\n",
    "#save_model_to_file(nlp_retrained_lg, 'saved_models/space_model_from_scratch.bin')\n",
    "\n",
    "loaded =  load_model_from_file('saved_models/space_model_from_scratch.bin',  \n",
    "                               spacy.blank('en'))\n",
    "\n",
    "stats_per_tag = compute_scores(test_examples, \n",
    "                               loaded,     ## Need to review before executing the cell\n",
    "                               PRED_LABELS_EQUIV_MAP)\n",
    "display_perf_stats_per_tag(stats_per_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHgKzcO_8u9l"
   },
   "source": [
    "# **Results**\n",
    "Scores on \n",
    "\n",
    "# **Conclusions**\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of SpaCy_Retraining_POC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
